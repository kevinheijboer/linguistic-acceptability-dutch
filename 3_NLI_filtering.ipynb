{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0100502",
   "metadata": {},
   "source": [
    "# **3. NLI Filtering**\n",
    "\n",
    "Labels aligned sentence pairs with edit types (e.g., addition, deletion, rewrite) using a Dutch Natural Language Inference (NLI) model.\n",
    "\n",
    "Edit Types:\n",
    "- **NO_CONTENT_CHANGE**\n",
    "- **CONTENT_ADDITION**\n",
    "- **CONTENT_DELETION**\n",
    "- **CONTENT_CHANGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "def classify_sentence_edits(input_file, \n",
    "                           output_file,\n",
    "                           sim_threshold=0.45):\n",
    "    \n",
    "    # Load the Dutch NLI pipeline\n",
    "    print(\"Loading Dutch NLI model...\")\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    nli = pipeline('text-classification',\n",
    "                   model='LoicDL/bert-base-dutch-cased-finetuned-snli',\n",
    "                   tokenizer='LoicDL/bert-base-dutch-cased-finetuned-snli',\n",
    "                   device=device)\n",
    "    \n",
    "    # Load the aligned pairs\n",
    "    print(f\"Loading aligned pairs from {input_file}...\")\n",
    "    df = pd.read_parquet(input_file)\n",
    "    \n",
    "    def classify_change(old, new, similarity):\n",
    "        # Classify the type of change between two sentences.\n",
    "        \n",
    "        # Handle None/empty cases\n",
    "        if not old or pd.isna(old) or old.strip() == '':\n",
    "            return 'CONTENT_ADDITION'\n",
    "        if not new or pd.isna(new) or new.strip() == '':\n",
    "            return 'CONTENT_DELETION'\n",
    "        \n",
    "        # Check for exact match\n",
    "        if old == new:\n",
    "            return 'NO_CONTENT_CHANGE'\n",
    "        \n",
    "        # Bidirectional NLI check\n",
    "        try:\n",
    "            # Forward direction: old → new\n",
    "            res_fwd = nli({\"text\": old, \"text_pair\": new}, truncation=True, top_k=None)[0]\n",
    "            # Backward direction: new → old  \n",
    "            res_bwd = nli({\"text\": new, \"text_pair\": old}, truncation=True, top_k=None)[0]\n",
    "            \n",
    "            fwd = res_fwd[\"label\"].upper()\n",
    "            bwd = res_bwd[\"label\"].upper()\n",
    "            \n",
    "            # Apply the classification logic\n",
    "            if fwd == 'ENTAILMENT' and bwd == 'ENTAILMENT':\n",
    "                return 'NO_CONTENT_CHANGE'\n",
    "            elif fwd == 'ENTAILMENT' and bwd != 'ENTAILMENT':\n",
    "                return 'CONTENT_DELETION'  \n",
    "            elif bwd == 'ENTAILMENT' and fwd != 'ENTAILMENT':\n",
    "                return 'CONTENT_ADDITION'\n",
    "            else:\n",
    "                return 'CONTENT_CHANGE'\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"NLI error: {e}\")\n",
    "            return 'CONTENT_CHANGE'\n",
    "    \n",
    "    # Process all records\n",
    "    records = []\n",
    "    \n",
    "    print(f\"Processing {len(df)} aligned pairs...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        old_sentence = row.get('old_sentence', '')\n",
    "        new_sentence = row.get('new_sentence', '')\n",
    "        similarity = row.get('similarity', row.get('similarity_score', 0))\n",
    "        \n",
    "        # Convert None to empty string\n",
    "        if pd.isna(old_sentence):\n",
    "            old_sentence = ''\n",
    "        if pd.isna(new_sentence):\n",
    "            new_sentence = ''\n",
    "        \n",
    "        # Split low similarity pairs into DELETION + ADDITION\n",
    "        if similarity is not None and similarity < sim_threshold and old_sentence and new_sentence:\n",
    "            # Create two separate records\n",
    "            records.append({\n",
    "                'isbn': row['isbn'],\n",
    "                'from_version': row['from_version'],\n",
    "                'to_version': row['to_version'],\n",
    "                'old_sentence': old_sentence,\n",
    "                'new_sentence': '',\n",
    "                'similarity': similarity,\n",
    "                'edit_type': 'CONTENT_DELETION'\n",
    "            })\n",
    "            records.append({\n",
    "                'isbn': row['isbn'],\n",
    "                'from_version': row['from_version'],\n",
    "                'to_version': row['to_version'],\n",
    "                'old_sentence': '',\n",
    "                'new_sentence': new_sentence,\n",
    "                'similarity': similarity,\n",
    "                'edit_type': 'CONTENT_ADDITION'\n",
    "            })\n",
    "        else:\n",
    "            # For high similarity pairs or actual deletions/additions, classify normally\n",
    "            edit_type = classify_change(old_sentence, new_sentence, similarity)\n",
    "            \n",
    "            records.append({\n",
    "                'isbn': row['isbn'],\n",
    "                'from_version': row['from_version'],\n",
    "                'to_version': row['to_version'],\n",
    "                'old_sentence': old_sentence,\n",
    "                'new_sentence': new_sentence,\n",
    "                'similarity': similarity,\n",
    "                'edit_type': edit_type\n",
    "            })\n",
    "    \n",
    "    # Create output dataframe\n",
    "    df_output = pd.DataFrame(records)\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"Saving results to {output_file}...\")\n",
    "    df_output.to_parquet(output_file, index=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nEdit type distribution:\")\n",
    "    print(df_output['edit_type'].value_counts())\n",
    "    print(f\"\\nTotal sentence pairs processed: {len(df_output)}\")\n",
    "    print(f\"Note: Low similarity pairs (< {sim_threshold}) were split into DELETION + ADDITION\")\n",
    "    \n",
    "    return df_output\n",
    "\n",
    "# Basic version without punctuation stripping\n",
    "df_labeled = classify_sentence_edits(\n",
    "    input_file=config['aligned_file'],\n",
    "    output_file=config['labeled_file'],\n",
    "    sim_threshold=0.45\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01258b",
   "metadata": {},
   "source": [
    "## **Extract micro-edits**\n",
    "\n",
    "This code filters out high-similarity NO_CONTENT_CHANGE sentence pairs (excluding exact matches) to extract likely linguistic/stylistic edits and saves them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de14b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_SIM = 0.9\n",
    "UPPER_SIM = 1     # exclude exact duplicates\n",
    "\n",
    "df = pd.read_parquet(config[\"labeled_file\"])\n",
    "\n",
    "df_q = (\n",
    "    df.query(\"edit_type == 'NO_CONTENT_CHANGE'\")\n",
    "      .query(f\"{LOWER_SIM} <= similarity < {UPPER_SIM}\")\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "df_q.to_parquet(config[\"nli_filtered_file\"], index=False)\n",
    "print(f\"Kept {len(df_q):,} micro-edits between {LOWER_SIM}–{UPPER_SIM} similarity\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
