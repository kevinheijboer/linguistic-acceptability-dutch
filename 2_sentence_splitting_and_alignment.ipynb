{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147ed400",
   "metadata": {},
   "source": [
    "# **2. Sentence Splitting and Alignment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1498d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88a5dc",
   "metadata": {},
   "source": [
    "## **Sentence Splitting**\n",
    "\n",
    "Reads cleaned book summaries, segments them into sentences using SaT (Segment Any Text) and saves the results as a parquet file (one sentence per row).\n",
    "\n",
    "Output:\n",
    "- **isbn**: book identifier\n",
    "- **version**: 1 | 2 | 3 (corresponding to different book description versions)\n",
    "- **sent_id**: sentence index within each version\n",
    "- **sentence**: the segmented sentence text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59ab4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting main sentence splitting process...\n",
      "Loading SaT model: sat-12l-sm\n",
      "Using GPU acceleration\n",
      "SaT model loaded successfully\n",
      "Preparing texts for processing...\n",
      "Processing 70876 texts in batches of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 4430/4430 [01:41<00:00, 43.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 585,824 sentences → data/sentences.parquet\n",
      "Processing Statistics:\n",
      "Model used: sat-12l-sm\n",
      "GPU acceleration: Yes\n",
      "Total texts processed: 70876\n",
      "Successful batches: 4430\n",
      "Failed batches: 0\n",
      "Total sentences generated: 585824\n",
      "Average sentences per text: 8.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "def split_sentences_sat(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    model_name: str = \"sat-3l-sm\",\n",
    "    use_gpu: bool = True,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    # Load cleaned data\n",
    "    df = pd.read_parquet(input_path)\n",
    "    # df = df.head(1500)\n",
    "\n",
    "    # Load SaT model\n",
    "    from wtpsplit import SaT\n",
    "    print(f\"Loading SaT model: {model_name}\")\n",
    "    sat = SaT(model_name)\n",
    "    \n",
    "    # Optional GPU acceleration\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        print(\"Using GPU acceleration\")\n",
    "        sat.half().to(\"cuda\")\n",
    "    else:\n",
    "        print(\"Using CPU processing\")\n",
    "        \n",
    "    print(\"SaT model loaded successfully\")\n",
    "        \n",
    "    # Helper functions\n",
    "    def prepare_texts_for_processing(df):\n",
    "        \"\"\"\n",
    "        Prepare texts for batch processing with SaT.\n",
    "        Returns list of (text, metadata) tuples.\n",
    "        \"\"\"\n",
    "        texts_with_metadata = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            for ver_idx, col in enumerate((\"version1\", \"version2\", \"version3\"), start=1):\n",
    "                text = row[col]\n",
    "                \n",
    "                # Skip empty/null texts\n",
    "                if pd.isna(text) or not text.strip():\n",
    "                    continue\n",
    "                \n",
    "                # Store text with its metadata\n",
    "                metadata = {\n",
    "                    \"isbn\": row[\"isbn\"],\n",
    "                    \"version\": ver_idx,\n",
    "                    \"original_text\": text\n",
    "                }\n",
    "                texts_with_metadata.append((text.strip(), metadata))\n",
    "        \n",
    "        return texts_with_metadata\n",
    "\n",
    "    def process_batch(texts_batch, metadata_batch):\n",
    "        \"\"\"\n",
    "        Process a batch of texts with SaT and return records.\n",
    "        \"\"\"\n",
    "        records = []\n",
    "    \n",
    "        # Get just the text strings for SaT\n",
    "        text_strings = [text for text, _ in texts_batch]\n",
    "        \n",
    "        # Process batch with SaT \n",
    "        sentence_lists = list(sat.split(text_strings))\n",
    "        \n",
    "        # Convert results to records\n",
    "        for (text, metadata), sentences in zip(texts_batch, sentence_lists):\n",
    "            for sent_id, sentence in enumerate(sentences):\n",
    "                # Clean up sentence \n",
    "                sentence = sentence.strip()\n",
    "                \n",
    "                if sentence:  # Skip empty sentences\n",
    "                    records.append({\n",
    "                        \"isbn\": metadata[\"isbn\"],\n",
    "                        \"version\": metadata[\"version\"],\n",
    "                        \"sent_id\": sent_id,\n",
    "                        \"sentence\": sentence,\n",
    "                    })\n",
    "        \n",
    "        return records\n",
    "\n",
    "    # Process all texts in batches \n",
    "    print(\"Preparing texts for processing...\")\n",
    "    texts_with_metadata = prepare_texts_for_processing(df)\n",
    "    \n",
    "    print(f\"Processing {len(texts_with_metadata)} texts in batches of {batch_size}\")\n",
    "    \n",
    "    all_records = []\n",
    "    processing_stats = {\n",
    "        'total_texts': len(texts_with_metadata),\n",
    "        'successful_batches': 0,\n",
    "        'failed_batches': 0,\n",
    "        'total_sentences': 0\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts_with_metadata), batch_size), desc=\"Processing batches\"):\n",
    "        batch = texts_with_metadata[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_records = process_batch(batch, None)  # metadata is in the batch tuples\n",
    "            all_records.extend(batch_records)\n",
    "            processing_stats['successful_batches'] += 1\n",
    "            processing_stats['total_sentences'] += len(batch_records)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process batch {i//batch_size + 1}: {e}\")\n",
    "            processing_stats['failed_batches'] += 1\n",
    "\n",
    "    # Save results and report \n",
    "    sent_df = pd.DataFrame.from_records(all_records)\n",
    "    sent_df.to_parquet(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(sent_df):,} sentences → {output_path}\")\n",
    "    print(f\"Processing Statistics:\")\n",
    "    print(f\"Model used: {model_name}\")\n",
    "    print(f\"GPU acceleration: {'Yes' if use_gpu and torch.cuda.is_available() else 'No'}\")\n",
    "    print(f\"Total texts processed: {processing_stats['total_texts']}\")\n",
    "    print(f\"Successful batches: {processing_stats['successful_batches']}\")\n",
    "    print(f\"Failed batches: {processing_stats['failed_batches']}\")\n",
    "    print(f\"Total sentences generated: {processing_stats['total_sentences']}\")\n",
    "    print(f\"Average sentences per text: {processing_stats['total_sentences'] / max(processing_stats['total_texts'], 1):.1f}\")\n",
    "\n",
    "\n",
    "# Run \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting main sentence splitting process...\")\n",
    "\n",
    "\n",
    "split_sentences_sat(\n",
    "    config[\"cleaned_file\"],\n",
    "    config[\"sentences_file\"], \n",
    "    model_name=\"sat-12l-sm\",  \n",
    "    use_gpu=True,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605f623",
   "metadata": {},
   "source": [
    "## **Sentence Alignment**\n",
    "\n",
    "This script aligns sentences between different versions of Dutch book summaries using a multi-stage approach:\n",
    "\n",
    "- **Stage 1**: Exact matches\n",
    "- **Stage 2**: High semantic + character similarity (via multilingual transformer & Levenshtein ratio)\n",
    "- **Stage 3**: Heuristic based on relative sentence position\n",
    "- **Stage 4**: Detects insertions and deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5366ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from Levenshtein import ratio as levenshtein_ratio\n",
    "from typing import List, Tuple, Optional\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DutchSentenceAligner:\n",
    "    def __init__(self, model_name='paraphrase-multilingual-mpnet-base-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.semantic_weight = 0.7\n",
    "        self.character_weight = 0.3\n",
    "        \n",
    "    def compute_similarity_matrix(self, sentences1: List[str], sentences2: List[str]) -> np.ndarray:\n",
    "        # Compute sentence embeddings\n",
    "        embeddings1 = self.model.encode(sentences1, convert_to_tensor=True, show_progress_bar=False)\n",
    "        embeddings2 = self.model.encode(sentences2, convert_to_tensor=True, show_progress_bar=False)\n",
    "        \n",
    "        # Compute semantic similarity\n",
    "        semantic_sim = torch.cosine_similarity(\n",
    "            embeddings1.unsqueeze(1), \n",
    "            embeddings2.unsqueeze(0), \n",
    "            dim=2\n",
    "        ).cpu().numpy()\n",
    "        \n",
    "        # Compute character-based similarity\n",
    "        char_sim = np.zeros((len(sentences1), len(sentences2)))\n",
    "        for i, s1 in enumerate(sentences1):\n",
    "            for j, s2 in enumerate(sentences2):\n",
    "                char_sim[i, j] = levenshtein_ratio(s1, s2)\n",
    "        \n",
    "        # Combine similarities\n",
    "        combined_sim = (self.semantic_weight * semantic_sim + \n",
    "                       self.character_weight * char_sim)\n",
    "        \n",
    "        return combined_sim\n",
    "    \n",
    "    def align_sentences(self, sentences1: List[str], sentences2: List[str], \n",
    "                       threshold: float = 0.7) -> List[dict]:\n",
    "        alignments = []\n",
    "        used_indices1 = set()\n",
    "        used_indices2 = set()\n",
    "        \n",
    "        # Stage 1: Exact matches\n",
    "        for i, s1 in enumerate(sentences1):\n",
    "            for j, s2 in enumerate(sentences2):\n",
    "                if j not in used_indices2 and s1 == s2:\n",
    "                    alignments.append({\n",
    "                        'old_sent_id': i,\n",
    "                        'new_sent_id': j,\n",
    "                        'old_sentence': s1,\n",
    "                        'new_sentence': s2,\n",
    "                        'similarity': 1.0,\n",
    "                        'alignment_type': 'exact'\n",
    "                    })\n",
    "                    used_indices1.add(i)\n",
    "                    used_indices2.add(j)\n",
    "                    break\n",
    "        \n",
    "        # Compute similarity matrix for remaining sentences\n",
    "        remaining1 = [(i, s) for i, s in enumerate(sentences1) if i not in used_indices1]\n",
    "        remaining2 = [(j, s) for j, s in enumerate(sentences2) if j not in used_indices2]\n",
    "        \n",
    "        if remaining1 and remaining2:\n",
    "            similarity_matrix = self.compute_similarity_matrix(\n",
    "                [s for _, s in remaining1],\n",
    "                [s for _, s in remaining2]\n",
    "            )\n",
    "            \n",
    "            # Stage 2: High similarity matches (greedy approach)\n",
    "            while True:\n",
    "                # Find the highest similarity pair\n",
    "                max_sim = similarity_matrix.max()\n",
    "                if max_sim < threshold:\n",
    "                    break\n",
    "                    \n",
    "                max_idx = np.unravel_index(similarity_matrix.argmax(), similarity_matrix.shape)\n",
    "                i_idx, j_idx = max_idx\n",
    "                \n",
    "                i_original = remaining1[i_idx][0]\n",
    "                j_original = remaining2[j_idx][0]\n",
    "                \n",
    "                alignments.append({\n",
    "                    'old_sent_id': i_original,\n",
    "                    'new_sent_id': j_original,\n",
    "                    'old_sentence': remaining1[i_idx][1],\n",
    "                    'new_sentence': remaining2[j_idx][1],\n",
    "                    'similarity': float(max_sim),\n",
    "                    'alignment_type': 'similar'\n",
    "                })\n",
    "                \n",
    "                # Mark as used\n",
    "                similarity_matrix[i_idx, :] = -1\n",
    "                similarity_matrix[:, j_idx] = -1\n",
    "                used_indices1.add(i_original)\n",
    "                used_indices2.add(j_original)\n",
    "        \n",
    "        # Stage 3: Order-based heuristic for remaining sentences\n",
    "        # If sentences maintain similar positions, they might be related even with lower similarity\n",
    "        remaining1 = [(i, s) for i, s in enumerate(sentences1) if i not in used_indices1]\n",
    "        remaining2 = [(j, s) for j, s in enumerate(sentences2) if j not in used_indices2]\n",
    "        \n",
    "        if remaining1 and remaining2:\n",
    "            for (i, s1) in remaining1:\n",
    "                # Look for sentences at similar relative positions\n",
    "                relative_pos1 = i / len(sentences1)\n",
    "                best_match = None\n",
    "                best_sim = 0\n",
    "                \n",
    "                for (j, s2) in remaining2:\n",
    "                    if j in used_indices2:\n",
    "                        continue\n",
    "                    relative_pos2 = j / len(sentences2)\n",
    "                    position_diff = abs(relative_pos1 - relative_pos2)\n",
    "                    \n",
    "                    # Only consider if positions are relatively close\n",
    "                    if position_diff <= 0.2:\n",
    "                        sim = self.compute_similarity_matrix([s1], [s2])[0, 0]\n",
    "                        if sim > best_sim and sim > 0.5:  # Lower threshold for position-based matching\n",
    "                            best_match = (j, s2, sim)\n",
    "                            best_sim = sim\n",
    "                \n",
    "                if best_match:\n",
    "                    j, s2, sim = best_match\n",
    "                    alignments.append({\n",
    "                        'old_sent_id': i,\n",
    "                        'new_sent_id': j,\n",
    "                        'old_sentence': s1,\n",
    "                        'new_sentence': s2,\n",
    "                        'similarity': float(sim),\n",
    "                        'alignment_type': 'position_based'\n",
    "                    })\n",
    "                    used_indices1.add(i)\n",
    "                    used_indices2.add(j)\n",
    "        \n",
    "        # Stage 4: Handle deletions and insertions\n",
    "        for i, s in enumerate(sentences1):\n",
    "            if i not in used_indices1:\n",
    "                alignments.append({\n",
    "                    'old_sent_id': i,\n",
    "                    'new_sent_id': None,\n",
    "                    'old_sentence': s,\n",
    "                    'new_sentence': None,\n",
    "                    'similarity': 0.0,\n",
    "                    'alignment_type': 'deletion'\n",
    "                })\n",
    "        \n",
    "        for j, s in enumerate(sentences2):\n",
    "            if j not in used_indices2:\n",
    "                alignments.append({\n",
    "                    'old_sent_id': None,\n",
    "                    'new_sent_id': j,\n",
    "                    'old_sentence': None,\n",
    "                    'new_sentence': s,\n",
    "                    'similarity': 0.0,\n",
    "                    'alignment_type': 'insertion'\n",
    "                })\n",
    "        \n",
    "        return alignments\n",
    "    \n",
    "    def align_versions(self, df: pd.DataFrame, isbn: int, from_version: int, \n",
    "                      to_version: int) -> List[dict]:\n",
    "        # Get sentences for each version\n",
    "        v1_sentences = df[(df['isbn'] == isbn) & \n",
    "                         (df['version'] == from_version)].sort_values('sent_id')\n",
    "        v2_sentences = df[(df['isbn'] == isbn) & \n",
    "                         (df['version'] == to_version)].sort_values('sent_id')\n",
    "        \n",
    "        # Extract sentence texts\n",
    "        sentences1 = v1_sentences['sentence'].tolist()\n",
    "        sentences2 = v2_sentences['sentence'].tolist()\n",
    "\n",
    "        # If either version has no sentences, nothing to align\n",
    "        if not sentences1 or not sentences2:\n",
    "            return []                    \n",
    "            \n",
    "        # Perform alignment\n",
    "        alignments = self.align_sentences(sentences1, sentences2)\n",
    "        \n",
    "        # Add metadata\n",
    "        for alignment in alignments:\n",
    "            alignment['isbn'] = isbn\n",
    "            alignment['from_version'] = from_version\n",
    "            alignment['to_version'] = to_version\n",
    "        \n",
    "        return alignments\n",
    "    \n",
    "    def align_all_books(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        all_alignments = []\n",
    "        isbns = df['isbn'].unique()\n",
    "\n",
    "        for isbn in tqdm(isbns, desc=\"Processing books\"):\n",
    "            versions = set(df.loc[df['isbn'] == isbn, 'version'])\n",
    "\n",
    "            # v1 and v2 present \n",
    "            if {1, 2}.issubset(versions):\n",
    "                all_alignments += self.align_versions(df, isbn, 1, 2)\n",
    "\n",
    "                # v3 as well? then also 2 → 3\n",
    "                if 3 in versions:\n",
    "                    all_alignments += self.align_versions(df, isbn, 2, 3)\n",
    "\n",
    "            # v2 missing but v1 & v3 present \n",
    "            elif {1, 3}.issubset(versions):\n",
    "                all_alignments += self.align_versions(df, isbn, 1, 3)\n",
    "\n",
    "\n",
    "        alignment_df = pd.DataFrame(all_alignments)\n",
    "        \n",
    "        # Reorder columns\n",
    "        column_order = ['isbn', 'from_version', 'to_version', 'old_sent_id', \n",
    "                       'new_sent_id', 'old_sentence', 'new_sentence', \n",
    "                       'similarity', 'alignment_type']\n",
    "        alignment_df = alignment_df[column_order]\n",
    "        \n",
    "        return alignment_df\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(config['sentences_file'])\n",
    "\n",
    "print(f\"Loaded {len(df)} sentences from {df['isbn'].nunique()} books.\")\n",
    "    \n",
    "# Initialize the aligner\n",
    "aligner = DutchSentenceAligner()\n",
    "\n",
    "# Perform alignment\n",
    "alignments = aligner.align_all_books(df)\n",
    "\n",
    "# Save results\n",
    "alignments.to_parquet(config['aligned_file'], index=False)\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total alignments: {len(alignments)}\")\n",
    "print(f\"Alignment type distribution:\")\n",
    "print(alignments['alignment_type'].value_counts())\n",
    "print(f\"\\nAverage similarity by type:\")\n",
    "print(alignments.groupby('alignment_type')['similarity'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
